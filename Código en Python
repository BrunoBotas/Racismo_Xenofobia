#Se descarga la biblioteca Spacy
!python -m spacy download es_core_news_sm # Se reincia el entorno de ejecución

#Se carga la librería
import spacy
nlp = spacy.load('es_core_news_sm')

#Se intalan los distintos paquetes que se van a utilizar
import sys
if 'google.colab' in sys.modules:
    !pip install emoji --upgrade
    !pip install pandas-profiling==2.*
    !pip install plotly==4.*
    !python -m spacy download es_core_news_sm
    !pip install pyldavis
    !pip install gensim
    !pip install nltk
    !pip install chart_studio
    !pip install --upgrade autopep8
    !pip install vaderSentiment
    !pip install datatable
    !pip install unidecode 
    !pip install --upgrade pip
    !pip install -U numpy #Volver a reiniciar el entorno de ejecución
    
#Se cargan las librerias
#Base and Cleaning 
import json
import requests
import pandas as pd
import numpy as np
import emoji
import regex
import re
import string
import pandas as pd
from collections import Counter
from random import seed
from unidecode import unidecode

#Visualizations
import plotly.express as px
import seaborn as sns
import matplotlib.pyplot as plt 
import pyLDAvis
import chart_studio
import chart_studio.plotly as py 
import chart_studio.tools as tls
import matplotlib.pyplot as plt
import pyLDAvis.gensim_models as gensimvis

#Natural Language Processing (NLP)
import spacy
import gensim
from spacy.tokenizer import Tokenizer
from gensim.corpora import Dictionary
from gensim.models.ldamulticore import LdaMulticore
from gensim import models
from gensim.models.coherencemodel import CoherenceModel
from gensim.parsing.preprocessing import STOPWORDS as SW
from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.model_selection import GridSearchCV
from pprint import pprint
from wordcloud import WordCloud, STOPWORDS
stopwords = set(STOPWORDS)
import nltk 
from nltk.tokenize import word_tokenize
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer 
nltk.download('stopwords')
from nltk.corpus import stopwords

#Pre-procesado de datos

#Se cargan los datos obtenidos en R
df_analisis = pd.read_csv('df_filtered_xenofobia.csv',sep=';')

#Se seleccionan aquellos en español
df_analisis = df_analisis[df_analisis['lang'] =="es"]

#Se quitan las stopwords
stop_words=stopwords.words('spanish')
print(stop_words)

#Se quitan aquellas stopwords que no se hayan eliminado en el proceso anterior
stop_words.extend(['y','des','ayyyy','etc','jo'])

#realizamos el proceso de lematización y añadimos de forma manual aquellas que no se hayan realizado
nlp = spacy.load('es_core_news_sm')
def lemma_words(text):
    lemmas = []
    doc= nlp(text)
    for token in doc: 
        if ((token.is_stop == False) and (token.is_punct == False)) and (token.pos_ != 'PRON'):
            lemmas.append(token.lemma_)
    lemmas= [i for i in lemmas if len(i) > 1]
    lemmas = [word for line in lemmas for word in line.split()]
    lemmas=[word for word in lemmas if word not in stop_words]
    lemmas=' '.join(lemmas)
    lemmas=unidecode(lemmas, "utf-8")
    lemmas=re.sub(r"\bllegado\b", "llegar", lemmas)
    lemmas=re.sub(r"\bdigais\b", "decir", lemmas)
    lemmas=re.sub(r"\bmigrant\b", "migrante", lemmas)
    lemmas=re.sub(r"\bespaa\b", "españa", lemmas)
    lemmas=re.sub(r"\bmigrant\b", "migrante", lemmas)
    lemmas=re.sub(r"\barabir\b", "arabe", lemmas)
    lemmas=re.sub(r"\bespaol\b", "español", lemmas)
    lemmas=re.sub(r"\balienigen\b", "alienigena", lemmas)
    lemmas=re.sub(r"\bpatero\b", "patera", lemmas)
    lemmas=re.sub(r"\bruso\b", "rusia", lemmas)
    lemmas=re.sub(r"\balmerio\b", "almeria", lemmas)
    lemmas=re.sub(r"\bbiografie\b", "biografia", lemmas)
    lemmas=re.sub(r"\bdio\b", "dios", lemmas)
    lemmas=re.sub(r"\bingl\b", "ingles", lemmas)
    lemmas=re.sub(r"\baparecero\b", "aparecer", lemmas)
    lemmas=re.sub(r"\bhebreir\b", "hebreo", lemmas)
    lemmas=re.sub(r"\barabar\b", "arabe", lemmas)
    lemmas=re.sub(r"\bquemado\b", "quemar", lemmas)
    lemmas=re.sub(r"\btoma\b", "tomas", lemmas)
    lemmas=re.sub(r"\bnios\b", "niños", lemmas)
    lemmas=re.sub(r"\bpai\b", "pais", lemmas)
    lemmas=re.sub(r"\bgracia\b", "gracias", lemmas)
    lemmas=re.sub(r"\bmarrueco\b", "marruecos", lemmas)
    lemmas=re.sub(r"\bisleto\b", "isleta", lemmas)
    lemmas=re.sub(r"\bnio\b", "niño", lemmas)
    lemmas=re.sub(r"\bcambio\b", "cambiar", lemmas)
    lemmas=re.sub(r"\bexpon\b", "exponer", lemmas)
    lemmas=re.sub(r"\bcompro\b", "comprar", lemmas)
    lemmas=re.sub(r"\bnurio\b", "nuria", lemmas)
    lemmas=re.sub(r"\btraido\b", "traer", lemmas)
    lemmas=re.sub(r"\bnecesitario\b", "necesitar", lemmas)
    lemmas=re.sub(r"\bspacio\b", "espacio", lemmas)
    lemmas=re.sub(r"\bexisten\b", "existir", lemmas)
    lemmas=re.sub(r"\borquestado\b", "orquestar", lemmas)
    lemmas=re.sub(r"\bbeneficiado\b", "beneficiar", lemmas)
    lemmas=re.sub(r"\bmen\b", "mena", lemmas)
    lemmas=re.sub(r"\bderechos\b", "derecho", lemmas)
    lemmas=re.sub(r"\bxenofobio\b", "xenofobia", lemmas)
    lemmas=re.sub(r"\bsharaui\b", "sahara", lemmas)
    lemmas=re.sub(r"\bquemao\b", "quemar", lemmas)
    lemmas=re.sub(r"\bacompaar\b", "acompañar", lemmas)
    lemmas=re.sub(r"\bacompaado\b", "acompañar", lemmas)
    lemmas=re.sub(r"\bucraniano\b", "ucrania", lemmas)
    lemmas=re.sub(r"\bao\b", "año", lemmas)
    lemmas=re.sub(r"\baos\b", "año", lemmas)
    lemmas=re.sub(r"\binvestigacion\b", "investigar", lemmas)
    lemmas = lemmas.split()
    lemmas=[word for word in lemmas if word not in stop_words]
    lemmas=' '.join(lemmas)
    return lemmas


df_analisis['lemmas'] = df_analisis['cleaned_text'].apply(lemma_words)
    
#Eliminamos las celdas vacias y los lemas duplicados
df_analisis.dropna(inplace=True)
df_analisis = df_analisis.drop_duplicates(subset='lemmas', keep="first")
df_analisis

#Generamos el csv de la columna lemas y el completo
df_analisis['lemmas'].to_csv('Lemmas_racismo_v9.csv')
df_analisis.to_csv('Racismo_limpio_v9.csv')

#Creación del Wordcloud

#Se define la función de crear una gráfica de wordcloud y se establecen las características
def plot_cloud(wordcloud):
    plt.figure(figsize=(10, 7))
    plt.imshow(wordcloud) 
    plt.axis("off");
    
#Pintamos el wordcloud
wordcloud = WordCloud(width = 3000, height = 2000, random_state=1, max_words=100, background_color='black', colormap='Set3', collocations=False, stopwords = STOPWORDS).generate(' '.join(df_analisis['lemmas']))
plot_cloud(wordcloud) 

#Análisis Descriptivo de N-Gramas

#Leemos el csv con los datos limpios y filtramos por la columna "Lemas"
df_analisis = pd.read_csv('Racismo_limpio_v9.csv')
dft= df_analisis['lemmas']
dft = [x for x in dft if str(x) != 'nan']
print(dft)

#Se calculan el Valor de TF-IDF de los unigramas
tfIdfVectorizer=TfidfVectorizer(use_idf=True, ngram_range=(1,1))
tfIdf = tfIdfVectorizer.fit_transform(dft)
names=tfIdfVectorizer.get_feature_names_out()
freqs = tfIdf.sum(axis=0).A1
result= dict(zip(names, freqs))
print(result)

# Se enseñan los 30 unigramas con mayor valor TF-IDF
from operator import itemgetter
i = 0
results_sorted=sorted(result.items(), key = itemgetter(1), reverse = True)
for key, value in results_sorted:
    i += 1
    if i == 31:
      break
    print(key, value)

#Se pintan los 30 unigramas con más TF-IDF
df_results=pd.DataFrame.from_dict(results_sorted).head(30)
plt.rcParams.update({'font.size': 20})
plt.figure(figsize=(15,7))
plt.bar(df_results[0],df_results[1])
plt.xticks(rotation=90)
plt.ylabel('TF-IDF Score')
plt.title('Unigramas más relevantes del corpus')

#Se realiza lo mismo con bigramas y trigramas

#Bigramas
tfIdfVectorizer_bi=TfidfVectorizer(use_idf=True, ngram_range=(2,2))
tfIdf_bi = tfIdfVectorizer_bi.fit_transform(dft)
names_bi=tfIdfVectorizer_bi.get_feature_names_out()
freqs_bi = tfIdf_bi.sum(axis=0).A1
result_bi= dict(zip(names_bi, freqs_bi))

from operator import itemgetter
i = 0
results_sorted=sorted(result_bi.items(), key = itemgetter(1), reverse = True)
for key, value in results_sorted:
    i += 1
    if i == 31:
      break
    print(key, value)
    
results_sorted_bi=sorted(result_bi.items(), key = itemgetter(1), reverse = True)
df_results_bi=pd.DataFrame.from_dict(results_sorted_bi).head(30)
plt.rcParams.update({'font.size': 20})
plt.figure(figsize=(15,7))
plt.bar(df_results_bi[0],df_results_bi[1])
plt.xticks(rotation=90)
plt.ylabel('TF-IDF Score')
plt.title('Bigramas más relevantes del corpus')

#Trigramas

tfIdfVectorizer_tri=TfidfVectorizer(use_idf=True, ngram_range=(3,3))
tfIdf_tri = tfIdfVectorizer_tri.fit_transform(dft)
names_tri= tfIdfVectorizer_tri.get_feature_names_out()
freqs_tri = tfIdf_tri.sum(axis=0).A1
result_tri = dict(zip(names_tri, freqs_tri))

from operator import itemgetter
i = 0
results_sorted=sorted(result_tri.items(), key = itemgetter(1), reverse = True)
for key, value in results_sorted:
    i += 1
    if i == 31:
      break
    print(key, value)
    
results_sorted_tri=sorted(result_tri.items(), key = itemgetter(1), reverse = True)
df_results_tri=pd.DataFrame.from_dict(results_sorted_tri).head(30)
plt.rcParams.update({'font.size': 20})
plt.figure(figsize=(15,7))
plt.bar(df_results_tri[0],df_results_tri[1])
plt.xticks(rotation=90)
plt.ylabel('TF-IDF Score')
plt.title('Trigramas más relevantes del corpus')

#Modelado de Tópicos

#Cargamos el csv con la limpieza hecha para realizar el modelado de tópicos
df_modelado = pd.read_csv('Racismo_limpio_v9.csv')
print(df_model.head())

#Tokenizamos la data
def tokenize(text):
    text = str(text)
    tokens = text.split() 
    return tokens

df_modelado['tokens'] = df_modelado['lemmas'].apply(tokenize)
print(df_modelado.head())

#Importamos una semilla para la reproducibilidad de los resultados
seed(24)
#Y creamos un dccionario a partir de los tokens encontrados
id2word = Dictionary(df_model['tokens'])

#Se filtran los extremos
id2word.filter_extremes(no_below=2, no_above=.95)

#Se crea el objeto del corpus
corpus = [id2word.doc2bow(d) for d in df_model['tokens']]

#Se procede a encontrar el valor óptimo de k
def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=2):
coherence_values_topic = []
    model_list_topic = []
    for num_topics in range(start, limit, step):
        model = LdaMulticore(corpus=corpus, num_topics=num_topics, id2word=id2word, passes=5)
        model_list_topic.append(model)
        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')
        coherence_values_topic.append(coherencemodel.get_coherence())

    return model_list_topic, coherence_values_topic
    
 #Se aplica la función definida en el paso anterior
 model_list_topic, coherence_values_topic = compute_coherence_values(dictionary=id2word,
                                                        corpus=corpus,
                                                        texts=df_model['tokens'],
                                                        start=2, limit=15, step=1) #Establecemos el rango de tópicos
                                    
#Se crea un csv con los valores coherencia del rango de tópicos establecido en el paso anterior
coherence_values_topic_df = pd.DataFrame(coherence_values_topic) 
coherence_values_topic_df.to_csv('coherence_values.csv', index=False)
coherence_values_topic_df= pd.read_csv('coherence_values.csv', index_col=False)
coherence_values_topic_df

#Se crea una lista ocn los valores de las columnas indicados
coherence_values_topic_df=coherence_values_topic_df.iloc[:, 0].tolist()

#Se dibuja una gráfica donde aparezcan elnúmero de tópicos y los valores coherencia para poder elegir el k óptimo
import matplotlib.path as mpath
plt.figure(figsize=(10,7))
estrella = mpath.Path.unit_regular_star(8)
circulo = mpath.Path.unit_circle()
verts = np.concatenate([circulo.vertices, estrella.vertices[::-1, ...]])
codes = np.concatenate([circulo.codes, estrella.codes])
cut_star = mpath.Path(verts, codes)
plt.plot([2,3,4,5,6,7,8,9,10,11,12,13,14],coherence_values_topic_df, '--r', marker="o", markersize=10, fillstyle='none')
plt.axvline(x=8, color='b', linestyle='--')
plt.plot(8, 0.410042, '--r', marker=cut_star, markersize=18)
plt.xlabel('Número de tópicos')
plt.ylabel('Indice de coherencia')

#Se genera el modelo LDA con el número de k elegido, en este caso k=8
K=8
model_k8 = gensim.models.LdaMulticore(corpus=corpus, 
                                        id2word=id2word, 
                                        num_topics=k,
                                        passes=5)
                                        
#Se procede a guardar y cargar todos los resultados obtenidos del modelo
model_k8.save("model_8_topics.model")
model_k8_load=LdaMulticore.load("model_8_topics.model")

#Se calcula el valor coherencia del modelo creado
coherence_model_k8 = CoherenceModel(model=model_k8_load, texts=df_model['tokens'], 
                                   dictionary=id2word, coherence='c_v')
coherence_model_k8 = coherence_model_k8.get_coherence()
print('\nCoherence Score: ', coherence_model_k8)

#Se observan las 12 palaras más utilizadas en cada tópico
print(model_k8_load.print_topics())
doc_lda = model_k8_load[corpus]
#Se filtra por palabras
words = [re.findall(r'"([^"]*)"',t[1]) for t in model_k8_load.print_topics()]
#Se crean los tópicos
topics = [' '.join(t[0:10]) for t in words]
for id, t in enumerate(topics): 
    print(f"------ Topic {id} ------")
    print(t, end="\n\n")

#Pintamos la distancia intertópica con pyLDAvis
pyLDAvis.enable_notebook()
gensimvis.prepare(model_k8_load, corpus, id2word)

#Se añade una columna de LDA features al modelo cargado
def document_to_lda_features(model_k8_load,document):
  topic_importance=np.array(model_k8_load.get_document_topics(document, minimum_probability=0))
  return topic_importance[:,1]

df_modelado['lda_features']=list(map(lambda doc: document_to_lda_features(model_k8_load,doc), corpus))

#Se tokeniza los datos
def topic_important(item_score):
    score=np.argmax(item_score, axis=0)
    return score

#Se aplica la tokenización
df_modelado['topic_dominant'] = df_model['lda_features'].apply(topic_important)
df_modelado

#Se genera un csv con los resultados
df_modelado.to_csv('topic_model_results.csv', index=False)

#Se saca el número de tweets por tópico
df_model["topic_dominant"].value_counts()

#Dibujamos esa distribución
plt.figure(figsize=(10,7))
ax=df_model["topic_dominant"].value_counts().sort_index().plot(kind='bar')
plt.ylabel('Frecuencia Absoluta')
plt.title('Distribución de tópicos en el corpus textual')
plt.show()

#En este apartado se procede a ver y pintar los bigramas de cada tópico con mayor TF-IDF

#Tópico 1
topic_1 = df_modelado[df_modelado['topic_dominant']==0] 
dft=topic_1['lemmas']
dft= [x for x in dft if str(x) != 'nan']
tfIdfVectorizer_bi=TfidfVectorizer(use_idf=True, ngram_range=(2,2))
tfIdf_bi = tfIdfVectorizer_bi.fit_transform(dft)
names_bi=tfIdfVectorizer_bi.get_feature_names_out()
freqs_bi = tfIdf_bi.sum(axis=0).A1
result_bi= dict(zip(names_bi, freqs_bi))
from operator import itemgetter
i = 0
results_sorted=sorted(result_bi.items(), key = itemgetter(1), reverse = True)
for key, value in results_sorted:
    i += 1
    if i == 11:
      break
    print(key, value)
#Se pinta
results_sorted_bi=sorted(result_bi.items(), key = itemgetter(1), reverse = True)
df_results_bi=pd.DataFrame.from_dict(results_sorted_bi).head(30)
plt.rcParams.update({'font.size': 20})
plt.figure(figsize=(15,7))
plt.bar(df_results_bi[0],df_results_bi[1])
plt.xticks(rotation=90)
plt.ylabel('TF-IDF Score')
plt.title('Bigramas más relevantes en el tópico 1')

#Tópico 2
topic_2 = df_modelado[df_modelado['topic_dominant']==1] 
dft=topic_2['lemmas']
tfIdfVectorizer_bi=TfidfVectorizer(use_idf=True, ngram_range=(2,2))
tfIdf_bi = tfIdfVectorizer_bi.fit_transform(dft)
names_bi=tfIdfVectorizer_bi.get_feature_names_out()
freqs_bi = tfIdf_bi.sum(axis=0).A1
result_bi= dict(zip(names_bi, freqs_bi))
from operator import itemgetter
i = 0
results_sorted=sorted(result_bi.items(), key = itemgetter(1), reverse = True)
for key, value in results_sorted:
    i += 1
    if i == 11:
      break
    print(key, value)
#Se pinta
results_sorted_bi=sorted(result_bi.items(), key = itemgetter(1), reverse = True)
df_results_bi=pd.DataFrame.from_dict(results_sorted_bi).head(30)
plt.rcParams.update({'font.size': 20})
plt.figure(figsize=(15,7))
plt.bar(df_results_bi[0],df_results_bi[1])
plt.xticks(rotation=90)
plt.ylabel('TF-IDF Score')
plt.title('Bigramas más relevantes del tópico 2')

#Tópico 3
topic_3 = df_modelado[df_modelado['topic_dominant']==2] 
dft=topic_3['lemmas']
dft= [x for x in dft if str(x) != 'nan']
tfIdfVectorizer_bi=TfidfVectorizer(use_idf=True, ngram_range=(2,2))
tfIdf_bi = tfIdfVectorizer_bi.fit_transform(dft)
names_bi=tfIdfVectorizer_bi.get_feature_names_out()
freqs_bi = tfIdf_bi.sum(axis=0).A1
result_bi= dict(zip(names_bi, freqs_bi))
from operator import itemgetter
i = 0
results_sorted=sorted(result_bi.items(), key = itemgetter(1), reverse = True)
for key, value in results_sorted:
    i += 1
    if i == 11:
      break
    print(key, value)
#Se pinta
results_sorted_bi=sorted(result_bi.items(), key = itemgetter(1), reverse = True)
df_results_bi=pd.DataFrame.from_dict(results_sorted_bi).head(30)
plt.rcParams.update({'font.size': 20})
plt.figure(figsize=(15,7))
plt.bar(df_results_bi[0],df_results_bi[1])
plt.xticks(rotation=90)
plt.ylabel('TF-IDF Score')
plt.title('Bigramas más relevantes del tópico 3')

#Tópico 4
topic_4 = df_modelado[df_modelado['topic_dominant']==3] 
dft=topic_4['lemmas']
tfIdfVectorizer_bi=TfidfVectorizer(use_idf=True, ngram_range=(2,2))
tfIdf_bi = tfIdfVectorizer_bi.fit_transform(dft)
names_bi=tfIdfVectorizer_bi.get_feature_names_out()
freqs_bi = tfIdf_bi.sum(axis=0).A1
result_bi= dict(zip(names_bi, freqs_bi))
from operator import itemgetter
i = 0
results_sorted=sorted(result_bi.items(), key = itemgetter(1), reverse = True)
for key, value in results_sorted:
    i += 1
    if i == 11:
      break
    print(key, value)
#Se pinta
results_sorted_bi=sorted(result_bi.items(), key = itemgetter(1), reverse = True)
df_results_bi=pd.DataFrame.from_dict(results_sorted_bi).head(30)
plt.rcParams.update({'font.size': 20})
plt.figure(figsize=(15,7))
plt.bar(df_results_bi[0],df_results_bi[1])
plt.xticks(rotation=90)
plt.ylabel('TF-IDF Score')
plt.title('Bigramas más relevantes del tópico 4')

#Tópico 5
topic_5 = df_modelado[df_modelado['topic_dominant']==4] 
dft=topic_5['lemmas']
tfIdfVectorizer_bi=TfidfVectorizer(use_idf=True, ngram_range=(2,2))
tfIdf_bi = tfIdfVectorizer_bi.fit_transform(dft)
names_bi=tfIdfVectorizer_bi.get_feature_names_out()
freqs_bi = tfIdf_bi.sum(axis=0).A1
result_bi= dict(zip(names_bi, freqs_bi))
from operator import itemgetter
i = 0
results_sorted=sorted(result_bi.items(), key = itemgetter(1), reverse = True)
for key, value in results_sorted:
    i += 1
    if i == 11:
      break
    print(key, value)
#Se pinta
results_sorted_bi=sorted(result_bi.items(), key = itemgetter(1), reverse = True)
df_results_bi=pd.DataFrame.from_dict(results_sorted_bi).head(30)
plt.rcParams.update({'font.size': 20})
plt.figure(figsize=(15,7))
plt.bar(df_results_bi[0],df_results_bi[1])
plt.xticks(rotation=90)
plt.ylabel('TF-IDF Score')
plt.title('Bigramas más relevantes del tópico 5')

#Tópico 6
topic_6 = df_modelado[df_modelado['topic_dominant']==5] 
dft=topic_6['lemmas']
tfIdfVectorizer_bi=TfidfVectorizer(use_idf=True, ngram_range=(2,2))
tfIdf_bi = tfIdfVectorizer_bi.fit_transform(dft)
names_bi=tfIdfVectorizer_bi.get_feature_names_out()
freqs_bi = tfIdf_bi.sum(axis=0).A1
result_bi= dict(zip(names_bi, freqs_bi))
from operator import itemgetter
i = 0
results_sorted=sorted(result_bi.items(), key = itemgetter(1), reverse = True)
for key, value in results_sorted:
    i += 1
    if i == 11:
      break
    print(key, value)
#Se pinta
results_sorted_bi=sorted(result_bi.items(), key = itemgetter(1), reverse = True)
df_results_bi=pd.DataFrame.from_dict(results_sorted_bi).head(30)
plt.rcParams.update({'font.size': 20})
plt.figure(figsize=(15,7))
plt.bar(df_results_bi[0],df_results_bi[1])
plt.xticks(rotation=90)
plt.ylabel('TF-IDF Score')
plt.title('Bigramas más relevantes del tópico 6')

#Tópico 7
topic_7 = df_modelado[df_modelado['topic_dominant']==6] 
dft=topic_7['lemmas']
tfIdfVectorizer_bi=TfidfVectorizer(use_idf=True, ngram_range=(2,2))
tfIdf_bi = tfIdfVectorizer_bi.fit_transform(dft)
names_bi=tfIdfVectorizer_bi.get_feature_names_out()
freqs_bi = tfIdf_bi.sum(axis=0).A1
result_bi= dict(zip(names_bi, freqs_bi))
from operator import itemgetter
i = 0
results_sorted=sorted(result_bi.items(), key = itemgetter(1), reverse = True)
for key, value in results_sorted:
    i += 1
    if i == 11:
      break
    print(key, value)
#Se pinta
results_sorted_bi=sorted(result_bi.items(), key = itemgetter(1), reverse = True)
df_results_bi=pd.DataFrame.from_dict(results_sorted_bi).head(30)
plt.rcParams.update({'font.size': 20})
plt.figure(figsize=(15,7))
plt.bar(df_results_bi[0],df_results_bi[1])
plt.xticks(rotation=90)
plt.ylabel('TF-IDF Score')
plt.title('Bigramas más relevantes del tópico 7')

#Tópico 8
topic_8 = df_modelado[df_modelado['topic_dominant']==7] 
dft=topic_8['lemmas']
tfIdfVectorizer_bi=TfidfVectorizer(use_idf=True, ngram_range=(2,2))
tfIdf_bi = tfIdfVectorizer_bi.fit_transform(dft)
names_bi=tfIdfVectorizer_bi.get_feature_names_out()
freqs_bi = tfIdf_bi.sum(axis=0).A1
result_bi= dict(zip(names_bi, freqs_bi))
from operator import itemgetter
i = 0
results_sorted=sorted(result_bi.items(), key = itemgetter(1), reverse = True)
for key, value in results_sorted:
    i += 1
    if i == 11:
      break
    print(key, value)
#Se pinta
results_sorted_bi=sorted(result_bi.items(), key = itemgetter(1), reverse = True)
df_results_bi=pd.DataFrame.from_dict(results_sorted_bi).head(30)
plt.rcParams.update({'font.size': 20})
plt.figure(figsize=(15,7))
plt.bar(df_results_bi[0],df_results_bi[1])
plt.xticks(rotation=90)
plt.ylabel('TF-IDF Score')
plt.title('Bigramas más relevantes del tópico 8')

#Se realiza el mismo proceso para los principales trigramas por tópico

#Tópico 1
topic_1 = df_modelado[df_modelado['topic_dominant']==0] 
dft=topic_1['lemmas']
dft= [x for x in dft if str(x) != 'nan']
tfIdfVectorizer_tri=TfidfVectorizer(use_idf=True, ngram_range=(3,3))
tfIdf_tri = tfIdfVectorizer_tri.fit_transform(dft)
names_tri= tfIdfVectorizer_tri.get_feature_names_out()
freqs_tri = tfIdf_tri.sum(axis=0).A1
result_tri = dict(zip(names_tri, freqs_tri))
from operator import itemgetter
i = 0
results_sorted=sorted(result_tri.items(), key = itemgetter(1), reverse = True)
for key, value in results_sorted:
    i += 1
    if i == 11:
      break
    print(key, value)
#Se pinta
results_sorted_tri=sorted(result_tri.items(), key = itemgetter(1), reverse = True)
df_results_tri=pd.DataFrame.from_dict(results_sorted_tri).head(30)
plt.rcParams.update({'font.size': 20})
plt.figure(figsize=(15,7))
plt.bar(df_results_tri[0],df_results_tri[1])
plt.xticks(rotation=90)
plt.ylabel('TF-IDF Score')
plt.title('Trigramas más relevantes del tópico 1')

#Tópico 2
topic_2 = df_modelado[df_modelado['topic_dominant']==1] 
dft=topic_2['lemmas']
tfIdfVectorizer_tri=TfidfVectorizer(use_idf=True, ngram_range=(3,3))
tfIdf_tri = tfIdfVectorizer_tri.fit_transform(dft)
names_tri= tfIdfVectorizer_tri.get_feature_names_out()
freqs_tri = tfIdf_tri.sum(axis=0).A1
result_tri = dict(zip(names_tri, freqs_tri))
from operator import itemgetter
i = 0
results_sorted=sorted(result_tri.items(), key = itemgetter(1), reverse = True)
for key, value in results_sorted:
    i += 1
    if i == 11:
      break
    print(key, value)
#Se pinta
results_sorted_tri=sorted(result_tri.items(), key = itemgetter(1), reverse = True)
df_results_tri=pd.DataFrame.from_dict(results_sorted_tri).head(30)
plt.rcParams.update({'font.size': 20})
plt.figure(figsize=(15,7))
plt.bar(df_results_tri[0],df_results_tri[1])
plt.xticks(rotation=90)
plt.ylabel('TF-IDF Score')
plt.title('Trigramas más relevantes del tópico 2')

#Tópico 3
topic_3 = df_modelado[df_modelado['topic_dominant']==2] 
dft=topic_3['lemmas']
tfIdfVectorizer_tri=TfidfVectorizer(use_idf=True, ngram_range=(3,3))
tfIdf_tri = tfIdfVectorizer_tri.fit_transform(dft)
names_tri= tfIdfVectorizer_tri.get_feature_names_out()
freqs_tri = tfIdf_tri.sum(axis=0).A1
result_tri = dict(zip(names_tri, freqs_tri))
from operator import itemgetter
i = 0
results_sorted=sorted(result_tri.items(), key = itemgetter(1), reverse = True)
for key, value in results_sorted:
    i += 1
    if i == 11:
      break
    print(key, value)
#Se pinta
results_sorted_tri=sorted(result_tri.items(), key = itemgetter(1), reverse = True)
df_results_tri=pd.DataFrame.from_dict(results_sorted_tri).head(30)
plt.rcParams.update({'font.size': 20})
plt.figure(figsize=(15,7))
plt.bar(df_results_tri[0],df_results_tri[1])
plt.xticks(rotation=90)
plt.ylabel('TF-IDF Score')
plt.title('Trigramas más relevantes del tópico 3')

#Tópico 4
topic_4 = df_modelado[df_modelado['topic_dominant']==3] 
dft=topic_4['lemmas']
tfIdfVectorizer_tri=TfidfVectorizer(use_idf=True, ngram_range=(3,3))
tfIdf_tri = tfIdfVectorizer_tri.fit_transform(dft)
names_tri= tfIdfVectorizer_tri.get_feature_names_out()
freqs_tri = tfIdf_tri.sum(axis=0).A1
result_tri = dict(zip(names_tri, freqs_tri))
from operator import itemgetter
i = 0
results_sorted=sorted(result_tri.items(), key = itemgetter(1), reverse = True)
for key, value in results_sorted:
    i += 1
    if i == 11:
      break
    print(key, value)
#Se pinta
results_sorted_tri=sorted(result_tri.items(), key = itemgetter(1), reverse = True)
df_results_tri=pd.DataFrame.from_dict(results_sorted_tri).head(30)
plt.rcParams.update({'font.size': 20})
plt.figure(figsize=(15,7))
plt.bar(df_results_tri[0],df_results_tri[1])
plt.xticks(rotation=90)
plt.ylabel('TF-IDF Score')
plt.title('Trigramas más relevantes del tópico 4')

#Tópico 5
topic_5 = df_modelado[df_modelado['topic_dominant']==4] 
dft=topic_5['lemmas']
tfIdfVectorizer_tri=TfidfVectorizer(use_idf=True, ngram_range=(3,3))
tfIdf_tri = tfIdfVectorizer_tri.fit_transform(dft)
names_tri= tfIdfVectorizer_tri.get_feature_names_out()
freqs_tri = tfIdf_tri.sum(axis=0).A1
result_tri = dict(zip(names_tri, freqs_tri))
from operator import itemgetter
i = 0
results_sorted=sorted(result_tri.items(), key = itemgetter(1), reverse = True)
for key, value in results_sorted:
    i += 1
    if i == 11:
      break
    print(key, value)
#Se pinta
results_sorted_tri=sorted(result_tri.items(), key = itemgetter(1), reverse = True)
df_results_tri=pd.DataFrame.from_dict(results_sorted_tri).head(30)
plt.rcParams.update({'font.size': 20})
plt.figure(figsize=(15,7))
plt.bar(df_results_tri[0],df_results_tri[1])
plt.xticks(rotation=90)
plt.ylabel('TF-IDF Score')
plt.title('Trigramas más relevantes del tópico 5')

#Tópico 6
topic_6 = df_modelado[df_modelado['topic_dominant']==5] 
dft=topic_6['lemmas']
tfIdfVectorizer_tri=TfidfVectorizer(use_idf=True, ngram_range=(3,3))
tfIdf_tri = tfIdfVectorizer_tri.fit_transform(dft)
names_tri= tfIdfVectorizer_tri.get_feature_names_out()
freqs_tri = tfIdf_tri.sum(axis=0).A1
result_tri = dict(zip(names_tri, freqs_tri))
from operator import itemgetter
i = 0
results_sorted=sorted(result_tri.items(), key = itemgetter(1), reverse = True)
for key, value in results_sorted:
    i += 1
    if i == 11:
      break
    print(key, value)
#Se pinta
results_sorted_tri=sorted(result_tri.items(), key = itemgetter(1), reverse = True)
df_results_tri=pd.DataFrame.from_dict(results_sorted_tri).head(30)
plt.rcParams.update({'font.size': 20})
plt.figure(figsize=(15,7))
plt.bar(df_results_tri[0],df_results_tri[1])
plt.xticks(rotation=90)
plt.ylabel('TF-IDF Score')
plt.title('Trigramas más relevantes del tópico 6')

#Tópico 7
topic_7 = df_modelado[df_modelado['topic_dominant']==6] 
dft=topic_7['lemmas']
tfIdfVectorizer_tri=TfidfVectorizer(use_idf=True, ngram_range=(3,3))
tfIdf_tri = tfIdfVectorizer_tri.fit_transform(dft)
names_tri= tfIdfVectorizer_tri.get_feature_names_out()
freqs_tri = tfIdf_tri.sum(axis=0).A1
result_tri = dict(zip(names_tri, freqs_tri))
from operator import itemgetter
i = 0
results_sorted=sorted(result_tri.items(), key = itemgetter(1), reverse = True)
for key, value in results_sorted:
    i += 1
    if i == 11:
      break
    print(key, value)
#Se pinta
results_sorted_tri=sorted(result_tri.items(), key = itemgetter(1), reverse = True)
df_results_tri=pd.DataFrame.from_dict(results_sorted_tri).head(30)
plt.rcParams.update({'font.size': 20})
plt.figure(figsize=(15,7))
plt.bar(df_results_tri[0],df_results_tri[1])
plt.xticks(rotation=90)
plt.ylabel('TF-IDF Score')
plt.title('Trigramas más relevantes del tópico 7')

#Tópico 8
topic_8 = df_modelado[df_modelado['topic_dominant']==7] 
dft=topic_8['lemmas']
tfIdfVectorizer_tri=TfidfVectorizer(use_idf=True, ngram_range=(3,3))
tfIdf_tri = tfIdfVectorizer_tri.fit_transform(dft)
names_tri= tfIdfVectorizer_tri.get_feature_names_out()
freqs_tri = tfIdf_tri.sum(axis=0).A1
result_tri = dict(zip(names_tri, freqs_tri))
from operator import itemgetter
i = 0
results_sorted=sorted(result_tri.items(), key = itemgetter(1), reverse = True)
for key, value in results_sorted:
    i += 1
    if i == 11:
      break
    print(key, value)
#Se pinta
results_sorted_tri=sorted(result_tri.items(), key = itemgetter(1), reverse = True)
df_results_tri=pd.DataFrame.from_dict(results_sorted_tri).head(30)
plt.rcParams.update({'font.size': 20})
plt.figure(figsize=(15,7))
plt.bar(df_results_tri[0],df_results_tri[1])
plt.xticks(rotation=90)
plt.ylabel('TF-IDF Score')
plt.title('Trigramas más relevantes del tópico 8')

#Análisis de Sentimiento

#Como se comenta en el documento, la limpieza de este apartado se realiza de una forma distinta, siendo todo el códio de R igual menos donde se quita los stopwords, puntuación y mayúsculas

#Se lee el nuevo corpus con la nuvea limpieza hecha y se eliminan los duplicados
df_sentiment =pd.read_excel('df_filtered_xenofobia_as.xlsx')
df_sentiment = df_sentiment.drop_duplicates()
df_sentiment

#También se procede a leer el documento con los resultados del modelado de tópicos
df_topics=pd.read_csv('topic_model_results.csv', sep=',',index_col=False)
df_topics

#Cogemos la todas las columnas y tranformamos el formato de la columna "Fecha"
df_topics=df_topics.iloc[:,[1,2,3,4,5,6,7,8,9,10,11,12,13]]
df_topics['date'] = pd.to_datetime(df_topics['fecha']) 
df_topics

#Juntamos los dos dataframes
df_model_sent=df_sentiment.merge(df_topics, how='left',on=['author_id', 'lang', 'fecha','retweet_number','reply_number','like_number','year','month']) 
df_model_sent

#Se eliminan duplicados de las columnas seleccionadas y se eliminana las filas vacias de la columan "topic_dominant"
df_model_sent=df_model_sent.drop_duplicates(subset=["cleaned_text_x", "cleaned_text_y", "author_id","fecha","lang","retweet_number","reply_number","like_number","year"]).reset_index(drop=True)
df_model_sent = df_model_sent.dropna(subset=['topic_dominant']) 
df_model_sent

#Tokenización
def tokenize(text):
    text = str(text)
    tokens = text.split()
    return tokens
df_model_sent ['tokens'] = df_model_sent ['cleaned_text_x'].apply(tokenize)
print(df_sentiment.head())

#Se elimina celdas vacias de la columna seleccionada
df_model_sent=df_model_sent.dropna(subset=['cleaned_text_x']).reset_index(drop=True)
df_model_sent

#Se añade una nueva columna con la libreria emoji utilizada en el dataset
import emoji
df_model_sent['cleaned_text_sent_emoji'] = df_model_sent['cleaned_text_x'].apply(lambda s: emoji.replace_emoji(s, '')) #añade una nueva colmna 
df_model_sent

#Se puntua el sentimiento de cada tweet 
def tweet_scores(data_frame): 
  
    sid_obj = SentimentIntensityAnalyzer()
    scores = [] 
    n = data_frame.shape[0]
    for i in range(n):
      sentence = data_frame.iloc[i]
      sentiment_dict = sid_obj.polarity_scores(sentence)
      compound = sentiment_dict['compound']
      scores.append(compound)
    return scores
    
#Se puntua la columna específica
df_model_sent['scores']=tweet_scores(df_model_sent['cleaned_text_sent_emoji']) 
df_model_sent['scores'].describe()

#Se cambia el formato a la columna fecha
df_sentiment['date'] = pd.to_datetime(df_sentiment['fecha']) 
df_sentiment

#Se dibuja un boxplot
df_model_sent.boxplot(column='scores', figsize=(12,8), showmeans=True, showfliers=False)

#Se agrupa por fecha y valores, añadiendo una columna fecha
df_bygroup=pd.DataFrame(df_model_sent.groupby(['date'])['scores'].agg('sum'))
df_bygroup['date']=df_bygroup.index
df_bygroup.to_csv('Racismo_AS_fecha.csv')
df_bygroup

#Se pintan los resultados
plt.figure(figsize=(15,7)) 
plt.rcParams['agg.path.chunksize'] = 10000
plt.plot(df_bygroup['date'],df_bygroup['scores'],color="darkblue")
plt.grid()
plt.show()

plt.hist(df_model_sent['scores'], bins=25) #Se pinta también un histograma

#Para finalizar se realizan los gráficos del analisis de sentimiento por tópico

#Tópico 1
topic_1_sentiment = df_model_sent[df_model_sent['topic_dominant']==0] 
topic_1_sentiment = topic_1_sentiment[topic_1_sentiment.date.notnull()]
df_bygroup=pd.DataFrame(topic_1_sentiment.groupby(['date'])['scores'].agg('sum'))
df_bygroup['date']=df_bygroup.index
df_bygroup.to_csv('Racismo_AS_1.csv')
df_bygroup
#Se pinta
plt.figure(figsize=(15,7))
plt.rcParams['agg.path.chunksize'] = 10000
plt.plot(df_bygroup['date'],df_bygroup['scores'],color="darkblue")
plt.grid()
plt.show()

#Tópico 2
topic_2_sentiment = df_model_sent[df_model_sent['topic_dominant']==1] 
topic_2_sentiment = topic_2_sentiment[topic_2_sentiment.date.notnull()]
df_bygroup=pd.DataFrame(topic_2_sentiment.groupby(['date'])['scores'].agg('sum'))
df_bygroup['date']=df_bygroup.index
df_bygroup.to_csv('Racismo_AS_2.csv')
df_bygroup
#Se pinta
plt.figure(figsize=(15,7))
plt.rcParams['agg.path.chunksize'] = 10000
plt.plot(df_bygroup['date'],df_bygroup['scores'],color="darkblue")
plt.grid()
plt.show()

#Tópico 3
topic_3_sentiment = df_model_sent[df_model_sent['topic_dominant']==2] 
topic_3_sentiment = topic_3_sentiment[topic_3_sentiment.date.notnull()]
df_bygroup=pd.DataFrame(topic_3_sentiment.groupby(['date'])['scores'].agg('sum'))
df_bygroup['date']=df_bygroup.index
df_bygroup.to_csv('Racismo_AS_3.csv')
df_bygroup
#Se Pinta
plt.figure(figsize=(15,7))
plt.rcParams['agg.path.chunksize'] = 10000
plt.plot(df_bygroup['date'],df_bygroup['scores'],color="darkblue")
plt.grid()
plt.show()

#Tópico 4
topic_4_sentiment = df_model_sent[df_model_sent['topic_dominant']==3] 
topic_4_sentiment = topic_4_sentiment[topic_4_sentiment.date.notnull()]
df_bygroup=pd.DataFrame(topic_4_sentiment.groupby(['date'])['scores'].agg('sum'))
df_bygroup['date']=df_bygroup.index
df_bygroup.to_csv('Racismo_AS_4.csv')
df_bygroup
#Se pinta
plt.figure(figsize=(15,7))
plt.rcParams['agg.path.chunksize'] = 10000
plt.plot(df_bygroup['date'],df_bygroup['scores'],color="darkblue")
plt.grid()
plt.show()

#Tópico 5
topic_5_sentiment = df_model_sent[df_model_sent['topic_dominant']==4] 
topic_5_sentiment = topic_5_sentiment[topic_5_sentiment.date.notnull()]
df_bygroup=pd.DataFrame(topic_5_sentiment.groupby(['date'])['scores'].agg('sum'))
df_bygroup['date']=df_bygroup.index
df_bygroup.to_csv('Racismo_AS_5.csv')
df_bygroup
#Se pinta
plt.figure(figsize=(15,7))
plt.rcParams['agg.path.chunksize'] = 10000
plt.plot(df_bygroup['date'],df_bygroup['scores'],color="darkblue")
plt.grid()
plt.show()

#Tópico 6
topic_6_sentiment = df_model_sent[df_model_sent['topic_dominant']==5] 
topic_6_sentiment = topic_6_sentiment[topic_6_sentiment.date.notnull()]
df_bygroup=pd.DataFrame(topic_6_sentiment.groupby(['date'])['scores'].agg('sum'))
df_bygroup['date']=df_bygroup.index
df_bygroup.to_csv('Racismo_AS_6.csv')
df_bygroup
#Se pinta
plt.figure(figsize=(15,7))
plt.rcParams['agg.path.chunksize'] = 10000
plt.plot(df_bygroup['date'],df_bygroup['scores'],color="darkblue")
plt.grid()
plt.show()

#Tópico 7
topic_7_sentiment = df_model_sent[df_model_sent['topic_dominant']==6] 
topic_7_sentiment = topic_7_sentiment[topic_7_sentiment.date.notnull()]
df_bygroup=pd.DataFrame(topic_7_sentiment.groupby(['date'])['scores'].agg('sum'))
df_bygroup['date']=df_bygroup.index
df_bygroup.to_csv('Racismo_AS_7.csv')
df_bygroup
#Se pinta
plt.figure(figsize=(15,7))
plt.rcParams['agg.path.chunksize'] = 10000
plt.plot(df_bygroup['date'],df_bygroup['scores'],color="darkblue")
plt.grid()
plt.show()

#Tópico 8
topic_8_sentiment = df_model_sent[df_model_sent['topic_dominant']==7] 
topic_8_sentiment = topic_8_sentiment[topic_8_sentiment.date.notnull()]
df_bygroup=pd.DataFrame(topic_8_sentiment.groupby(['date'])['scores'].agg('sum'))
df_bygroup['date']=df_bygroup.index
df_bygroup.to_csv('Racismo_AS_8.csv')
df_bygroup
#Se pinta
plt.figure(figsize=(15,7))
plt.rcParams['agg.path.chunksize'] = 10000
plt.plot(df_bygroup['date'],df_bygroup['scores'],color="darkblue")
plt.grid()
plt.show()






























