#Primero procedemos a intalar los paquetes necesarios y a cargar las librerias que utilizaremos durante el código realizado en R
install.packages("academictwitteR")

library(academictwitteR)
library(ggplot2)
library(tidyverse)
library(data.table)
library(textclean)
library(tm)
library(stringr)
library(dplyr)
library(rtweet)
set_bearer()

#Añadimos una línea donde aparezca (TWITTER_BEARER=Contraseña), para poder acceder a la API de Twitter

#Se reinicia el R, para acceder a la API

library(academictwitteR) #Se carga la libreria previamente descargada

# Preparación de Datos

#Descarga de datos donde añadimos la franja de tiempo, el pais, el idioma y las palabras clave
tweets <-
  get_all_tweets(
    query = c("moro", "invasión", "sudaca", "árabe", "migración", "migrantes", "inmigrantes","refugiado", "gitano", "patera", "mena", "panchito", "indio", "negrata","Jihad", "gabacho" ),
    start_tweets = "2020-01-01T00:00:00Z",
    end_tweets = "2022-12-31T00:00:00Z",
    bearer_token = get_bearer(),
    bind_tweets=FALSE,
    country = "ES", 
    lang = "es",
    file = "tweets_racismo",
    data_path="data_complete_racismo/",
    n = 1000000,
  )

#Se unifican todos lo tweets bajo el nombre de tweets_racismo
tweets_racismo <- bind_tweets(data_path = "data_complete_racismo/")

#Se formatean las columnas del documento creado para el análisis posterior

df<-data.frame(author_id=tweets_racismo$author_id, text=tweets_racismo$text, fecha=tweets_racismo$created_at, lang=tweets_racismo$lang, retweet_number=tweets_racismo$public_metrics$retweet_count, reply_number=tweets_racismo$public_metrics$reply_count,like_number=tweets_racismo$public_metrics$like_count)
df$hashtags=lapply(tweets_racismo$entities$hashtags, function(x) if (length(x) == 0) NA else  toString(unlist(`[`(x, c('tag')))) )
df$users=lapply(tweets_racismo$entities$mentions, function(x) if (length(x) == 0) NA else  array(unlist(`[`(x, c('username')))) )
df$fecha <- as.Date(df$fecha, format = "%Y-%m-%d")
df$year <- format(df$fecha, "%Y")
df$month <- format(df$fecha, "%m")
df$day <- format(df$fecha, "%d")

#Realizamos los principales análisis

#Sacamos la cantidad de retweets que tienen los diversos tweets del corpus

number_tweets_bynumberretweets<-df %>% 
  group_by(retweet_number) %>% 
  summarise(count = n())

#Sacamos también el número de tweets por fecha y idioma

number_tweets_bydate<-df %>% 
  group_by(fecha) %>% 
  summarise(count = n())

number_tweets_bylang<-df %>% 
  group_by(lang) %>% 
  summarise(count = n())

#Vemos la lista de los principales hashtags más utilizados

all_hashtags <- unlist(
  regmatches(df$hashtags,  gregexpr('\\w+', df$hashtags)))

freq_count <- as.data.frame(table(all_hashtags))

freq_count<-freq_count[order(freq_count$Freq, decreasing = TRUE),]

#Descargamos y cargamos otra libreria para poder ver los principales emoticonos utilizados
devtools::install_github("hadley/emo")

library(emo)
emojis<-df %>%
  mutate(emoji = ji_extract_all(text)) %>%
  tidyr::unnest(cols = c(emoji)) %>%
  count(emoji, sort = TRUE) %>%
  top_n(20)


#Pre-procesamiento

#Se establecen una serie de reglas personalizadas para la transformación de texto
 custom_rules <- "ñ > \\~;
                 Ñ > \\^;
                 ::Latin-ASCII;
                 \\~ > ñ;
                 \\^ > Ñ"

#Generamos las distintas stopwords
stopw<- stopwords("spanish")
clean <- function(x){
  if (!(is.na(x))){x <- gsub("http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+", "", x)} #Remove URLs
  if (!(is.na(x))){x <- gsub("@\\w+", "", x)} #Remove mentions}
  if (!(is.na(x))){x <- gsub("#\\w+", "", x)} #Remove hashtags}
  if (!(is.na(x))){x <- gsub("\\d+\\w*\\d*", "", x)}
  
  if (!(is.na(x))){x<- stringi::stri_trans_general(x, id = custom_rules, rules = TRUE)}
  if (!(is.na(x))){x <- gsub("[[:punct:]]", " ", x) } #Remove punctuation
  if (!(is.na(x))){x <-tolower(x)}
  if (!(is.na(x))){x <-iconv(x, "latin1", "ASCII", sub="")}

  if (!(is.na(x))){x <-removeNumbers(x)}
  if (!(is.na(x))){ x <-removeWords(x,stop)}
  if (!(is.na(x))){x <-gsub('\\b+RT', '', x)} ## Remove RT}
  
  if (!(is.na(x))){x <- gsub('\\b\\w{1,2}\\s','',x)}
  if (!(is.na(x))){x <- str_replace(gsub("\\s+", " ", str_trim(x)), "B", "b")} #multiple spaces to single space
  return(x)
}

#Sacamos el corpus limpio
df$cleaned_text<-clean(df$text)
df$cleaned_text<-apply(df[c('text')],MARGIN = 1, FUN = clean)
 df_filtered<-df[!(is.na(df$cleaned_text) | df$cleaned_text==""), ]
 
 #Procedemos a sacar un csv con todos los datos para empezar nuestro análisi en Python
 
 drop <- c("hashtags","users","text")
 df_filtered = df_filtered[,!(names(df_filtered) %in% drop)]
 write.csv2(df,"df_xenofobia.csv", row.names = FALSE)
